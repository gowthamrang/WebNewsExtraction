{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of webpages: 269\n",
      "Number of nodes: 65194\n",
      "Number of Title Nodes: 266\n",
      "Number of Content Nodes: 3803\n",
      "Number of Date Nodes: 204\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis\n",
    "\n",
    "#Number of unique websites\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read train.csv\n",
    "#read fids\n",
    "\n",
    "\n",
    "#find number of unique domains\n",
    "#\n",
    "#How varied is the Dataset ?\n",
    "#Any interesting statistics ?\n",
    "\n",
    "import os \n",
    "import numpy as np\n",
    "def load(direc):\n",
    "    examples = []\n",
    "    fids  = []\n",
    "    values = []\n",
    "    for each in os.listdir(direc):        \n",
    "        if '.npy' in each:\n",
    "            fids.append(each.split('_')[1][:-4])\n",
    "            examples.extend(np.load(direc+'/'+each))\n",
    "            with open(direc+'/'+each[:-4]+'.json', 'r') as infile:\n",
    "                values.extend(json.load(infile))\n",
    "    return examples,fids,values\n",
    "\n",
    "\n",
    "trainexamples, trainfids, trainvalues = load('Contentfeature/train')\n",
    "trainexamples = np.array(trainexamples)\n",
    "\n",
    "print 'Number of webpages: %d' %(len(trainfids))\n",
    "print 'Number of nodes: %d' %(trainexamples.shape[0])\n",
    "print 'Number of Title Nodes: %d'%(len(filter(lambda x: x[0]==1 , trainexamples)))\n",
    "print 'Number of Content Nodes: %d' %(len(filter(lambda x: x[0]==2 , trainexamples)))\n",
    "print 'Number of Date Nodes: %d' %(len(filter(lambda x: x[0]==3 , trainexamples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique domains 239 ; Average number of articles per domain 1.130\n",
      "11alive.com 1\n",
      "4029tv.com 1\n",
      "6abc.com 1\n",
      "abc-7.com 1\n",
      "abc.net.au 2\n",
      "abc13.com 1\n",
      "abc27.com 1\n",
      "abcnews.go.com 2\n",
      "abcnews4.com 1\n",
      "alaskadispatch.com 1\n",
      "Features that are mostly 0 [(0.0, 55, u'mspam'), (1.0, 18, u'ancestorTag=form'), (1.0, 36, u'ancestorTag=ssname'), (1.0, 43, u'ancestorTag=u'), (1.0, 47, u'data-font-size=12px'), (1.0, 48, u'data-font-size=14px'), (1.0, 49, u'data-font-size=16px'), (1.0, 67, u'next_tag=comments'), (1.0, 72, u'next_tag=figcaption'), (1.0, 87, u'next_tag=include')]\n",
      "-*-*-*-*-*-*-*-*-*-*\n",
      "Features that are always active [(5552.0, 143, u'parent_tag=p'), (9749.0, 35, u'ancestorTag=span'), (9749.0, 146, u'parent_tag=span'), (18520.107491872615, 54, u'functional_word_ratio'), (36235.0, 0, u'ancestorTag=a'), (36235.0, 116, u'parent_tag=a'), (44657.0, 56, u'next_tag=__empty__'), (46503.0, 156, u'previous_tag=__empty__'), (180056.0, 53, u'functional_word_count'), (368296.0, 213, u'word_count')]\n"
     ]
    }
   ],
   "source": [
    "infile =  open('train/field.csv','r')\n",
    "websites = {}\n",
    "for each in infile.readlines():\n",
    "    domain = each.strip().split(',')[2].split('/')[:3][-1]\n",
    "    if domain.split('.')[0] == 'www':\n",
    "        domain = '.'.join(domain.split('.')[1:])\n",
    "    if each.strip().split(',')[0] in trainfids:\n",
    "        websites[domain] = websites[domain]+1 if domain in websites else 1\n",
    "print 'Number of unique domains %d ; Average number of articles per domain %.3f' %(len(websites),sum(e for e in websites.values())*1.0/len(websites))\n",
    "for each in sorted(websites.keys())[:10]:\n",
    "    print each, websites[each]\n",
    "    \n",
    "res = []\n",
    "k,v = zip(*d.items())    \n",
    "for col,key in sorted(zip(v,k)):\n",
    "    res.append((np.sum(X[:,col]),col,key))\n",
    "print 'Features that are mostly 0 %s' %(sorted(res)[:10])\n",
    "print '-*'*10\n",
    "print 'Features that are always active %s' %(sorted(res)[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#expolaratory analysis\n",
    "    #does feature space capture \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "pca = PCA(n_components=2)\n",
    "trainfeatures = map(lambda x:x[1:] , trainexamples)\n",
    "trainfeatures1 = normalize(trainfeatures)\n",
    "X = pca.fit_transform(trainfeatures1)\n",
    "assert(X.shape[1] == 2)\n",
    "\n",
    "d = {0:[],1:[],2:[],3:[]}\n",
    "for i,each in enumerate(trainexamples):\n",
    "    d[each[0]].append(X[i,:])\n",
    "\n",
    "x,y = zip(*d[0])\n",
    "plt.plot(x,y,'bo')\n",
    "x,y = zip(*d[1])\n",
    "plt.plot(x,y,'go')\n",
    "x,y = zip(*d[2])\n",
    "plt.plot(x,y,'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naieve Base line all Content: 0.1102\n",
      "(65194, 1) (65194,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "trainlabels = map(lambda x: x[0]==2, trainexamples)\n",
    "trainfeatures = map(lambda x: x[1:], trainexamples)\n",
    "X = np.matrix(trainfeatures)\n",
    "Y = np.array(trainlabels)\n",
    "print 'Naieve Base line all Content: %.4f' %(f1_score([1]*len(Y),Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### SVC  ###############################\n",
      "#################### LOGISTIC REGRESSION ###############################\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] C=1000, class_weight={0: 1, 1: 15} ..............................\n",
      "[CV] ..................... C=1000, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 15} ..............................\n",
      "[CV] ..................... C=1000, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 15} ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1000, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 15} ..............................\n",
      "[CV] ..................... C=1000, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 15} ..............................\n",
      "[CV] ..................... C=1000, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 5} .................................\n",
      "[CV] ........................ C=50, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 5} .................................\n",
      "[CV] ........................ C=50, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 5} .................................\n",
      "[CV] ........................ C=50, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 5} .................................\n",
      "[CV] ........................ C=50, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 5} .................................\n",
      "[CV] ........................ C=50, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=0.7, class_weight={0: 1, 1: 5} ................................\n",
      "[CV] ....................... C=0.7, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=0.7, class_weight={0: 1, 1: 5} ................................\n",
      "[CV] ....................... C=0.7, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=0.7, class_weight={0: 1, 1: 5} ................................\n",
      "[CV] ....................... C=0.7, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=0.7, class_weight={0: 1, 1: 5} ................................\n",
      "[CV] ....................... C=0.7, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=0.7, class_weight={0: 1, 1: 5} ................................\n",
      "[CV] ....................... C=0.7, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=100, class_weight={0: 1, 1: 15} ...............................\n",
      "[CV] ...................... C=100, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=100, class_weight={0: 1, 1: 15} ...............................\n",
      "[CV] ...................... C=100, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=100, class_weight={0: 1, 1: 15} ...............................\n",
      "[CV] ...................... C=100, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=100, class_weight={0: 1, 1: 15} ...............................\n",
      "[CV] ...................... C=100, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=100, class_weight={0: 1, 1: 15} ...............................\n",
      "[CV] ...................... C=100, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 15} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 15} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 15} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 15} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 15} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 15} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 10} ................................\n",
      "[CV] ....................... C=50, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 10} ................................\n",
      "[CV] ....................... C=50, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 10} ................................\n",
      "[CV] ....................... C=50, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 10} ................................\n",
      "[CV] ....................... C=50, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=50, class_weight={0: 1, 1: 10} ................................\n",
      "[CV] ....................... C=50, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.6, class_weight={0: 1, 1: 10} ...............................\n",
      "[CV] ...................... C=0.6, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.6, class_weight={0: 1, 1: 10} ...............................\n",
      "[CV] ...................... C=0.6, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.6, class_weight={0: 1, 1: 10} ...............................\n",
      "[CV] ...................... C=0.6, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.6, class_weight={0: 1, 1: 10} ...............................\n",
      "[CV] ...................... C=0.6, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.6, class_weight={0: 1, 1: 10} ...............................\n",
      "[CV] ...................... C=0.6, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.9, class_weight={0: 1, 1: 1} ................................\n",
      "[CV] ....................... C=0.9, class_weight={0: 1, 1: 1} -   0.1s\n",
      "[CV] C=0.9, class_weight={0: 1, 1: 1} ................................\n",
      "[CV] ....................... C=0.9, class_weight={0: 1, 1: 1} -   0.1s\n",
      "[CV] C=0.9, class_weight={0: 1, 1: 1} ................................\n",
      "[CV] ....................... C=0.9, class_weight={0: 1, 1: 1} -   0.1s\n",
      "[CV] C=0.9, class_weight={0: 1, 1: 1} ................................\n",
      "[CV] ....................... C=0.9, class_weight={0: 1, 1: 1} -   0.1s\n",
      "[CV] C=0.9, class_weight={0: 1, 1: 1} ................................\n",
      "[CV] ....................... C=0.9, class_weight={0: 1, 1: 1} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 10} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 10} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 10} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 10} .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  41 jobs       | elapsed:    3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=0.001, class_weight={0: 1, 1: 10} .............................\n",
      "[CV] .................... C=0.001, class_weight={0: 1, 1: 10} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 5} ...............................\n",
      "[CV] ...................... C=1000, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 5} ...............................\n",
      "[CV] ...................... C=1000, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 5} ...............................\n",
      "[CV] ...................... C=1000, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 5} ...............................\n",
      "[CV] ...................... C=1000, class_weight={0: 1, 1: 5} -   0.1s\n",
      "[CV] C=1000, class_weight={0: 1, 1: 5} ...............................\n",
      "[CV] ...................... C=1000, class_weight={0: 1, 1: 5} -   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 0.00 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.562 (std: 0.004)\n",
      "Parameters: {'C': 50, 'class_weight': {0: 1, 1: 10}}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.562 (std: 0.004)\n",
      "Parameters: {'C': 0.6, 'class_weight': {0: 1, 1: 10}}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.555 (std: 0.005)\n",
      "Parameters: {'C': 0.001, 'class_weight': {0: 1, 1: 10}}\n",
      "\n",
      "Continue ??\n",
      "d\n",
      "#################### RANDOM FOREST ###############################\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 -   0.1s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 -   0.1s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=gini, max_depth=3, min_samples_leaf=4 -   0.1s\n",
      "[CV] min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 \n",
      "[CV]  min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 -   0.2s\n",
      "[CV] min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 \n",
      "[CV]  min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 -   0.2s\n",
      "[CV] min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 \n",
      "[CV]  min_samples_split=4, bootstrap=True, criterion=gini, max_depth=None, min_samples_leaf=7 -   0.2s\n",
      "[CV] min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 -   0.2s\n",
      "[CV] min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 -   0.1s\n",
      "[CV] min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 \n",
      "[CV]  min_samples_split=6, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=10 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 \n",
      "[CV]  min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 \n",
      "[CV]  min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 \n",
      "[CV]  min_samples_split=5, bootstrap=True, criterion=gini, max_depth=3, min_samples_leaf=6 -   0.1s\n",
      "[CV] min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 \n",
      "[CV]  min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 -   0.2s\n",
      "[CV] min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 \n",
      "[CV]  min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 -   0.2s\n",
      "[CV] min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 \n",
      "[CV]  min_samples_split=7, bootstrap=True, criterion=entropy, max_depth=None, min_samples_leaf=9 -   0.2s\n",
      "[CV] min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=1, bootstrap=True, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=None, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.2s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 \n",
      "[CV]  min_samples_split=2, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=5 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 \n",
      "[CV]  min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 \n",
      "[CV]  min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 -   0.1s\n",
      "[CV] min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 \n",
      "[CV]  min_samples_split=5, bootstrap=False, criterion=entropy, max_depth=3, min_samples_leaf=1 -   0.1s\n",
      "RandomizedSearchCV took 4.67 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.441 (std: 0.075)\n",
      "Parameters: {'min_samples_split': 1, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 5}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.441 (std: 0.075)\n",
      "Parameters: {'min_samples_split': 2, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 5}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.441 (std: 0.075)\n",
      "Parameters: {'min_samples_split': 5, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 1}\n",
      "\n",
      "BestParams and score\n",
      "[({'C': 50, 'class_weight': {0: 1, 1: 10}}, 0.56179279676893812), ({'min_samples_split': 1, 'bootstrap': False, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 5}, 0.44062303958369115)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    4.5s finished\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('Contentfeature/train/featurenames.json') as infile:\n",
    "    featurenames = json.load(infile)\n",
    "assert(len(featurenames) == len(trainfeatures[0]))\n",
    "\n",
    "\n",
    "################ ML methods ######################################\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from operator import itemgetter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "d = dict(zip(featurenames,range(len(featurenames))))\n",
    "# print '-----------------'*10\n",
    "# print 'FeatureNames'\n",
    "# for each in d.keys():\n",
    "#     print each\n",
    "# print '-----------------'*10\n",
    "###################################################################################################################\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        if i == 0:\n",
    "            bestparams = (score.parameters,score.mean_validation_score)\n",
    "        print(\"\")\n",
    "    return bestparams\n",
    "\n",
    "def getf1score(estimator, X, Y):\n",
    "    #return recall_score(estimator.predict(X),Y)\n",
    "    return f1_score(estimator.predict(X),Y)\n",
    "\n",
    "\n",
    "\n",
    "clf1 = SVC(C=1.0)\n",
    "clf2 = LogisticRegression(penalty='l2', random_state=4,C=1.0)\n",
    "clf3 = RandomForestClassifier(n_estimators=20)\n",
    "s3fold = StratifiedKFold(y=Y, n_folds=5,\n",
    "                                         shuffle=True, random_state=2)\n",
    "bestparams = []\n",
    "\n",
    "C = [0.001, 0.001, 0.1, 0.7, 0.5, 0.6, 0.9, 1, 2, 5, 7, 10,50, 70, 100,1000]\n",
    "gamma = [0.00001, 0.0001,0.001, 0.01, 0.1]\n",
    "class_weights = [{0:1,1:1},{0:1,1:5},{0:1,1:10},{0:1,1:15},{0:1,1:20}]\n",
    "\n",
    "param_dist = {\"C\": C, \"gamma\":gamma, \"class_weight\":class_weights}\n",
    "\n",
    "Xnew = X[:,d['word_count']]\n",
    "\n",
    "\n",
    "\n",
    "print '#################### SVC  ###############################'\n",
    "np.random.seed(5)\n",
    "# # run randomized search\n",
    "n_iter_search = 20\n",
    "# random_search = RandomizedSearchCV(clf1, param_distributions=param_dist, scoring=getf1score,\n",
    "#                                    verbose=2, cv=s3fold )\n",
    "# start = time()\n",
    "# random_search.fit(Xnew, Y)\n",
    "# print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "#       \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "# bestparams.append(report(random_search.grid_scores_))\n",
    "# print 'Continue ??'\n",
    "# raw_input()\n",
    "\n",
    "param_dist = {\"C\": C,\"class_weight\": class_weights}\n",
    "print '#################### LOGISTIC REGRESSION ###############################'\n",
    "random_search = RandomizedSearchCV(clf2, param_distributions=param_dist, scoring=getf1score,\n",
    "                                   verbose=2 ,cv=s3fold)\n",
    "\n",
    "random_search.fit(Xnew, Y)\n",
    "start = time()\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "bestparams.append(report(random_search.grid_scores_))\n",
    "print 'Continue ??'\n",
    "raw_input()\n",
    "\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "print '#################### RANDOM FOREST ###############################'\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"min_samples_split\": sp_randint(1, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]\n",
    "             }\n",
    "random_search = RandomizedSearchCV(clf3, param_distributions=param_dist, scoring=getf1score,\n",
    "                                   verbose=2 )\n",
    "\n",
    "\n",
    "start = time()\n",
    "random_search.fit(Xnew, Y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "bestparams.append(report(random_search.grid_scores_))\n",
    "print 'BestParams and score'\n",
    "print bestparams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
