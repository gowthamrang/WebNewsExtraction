{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing map\n",
      "Prior's Influence\n",
      "Original Parameters (m,c) = (1,1)\n",
      "---***---Weighing too much over an incorrect prior, High variance---***---\n",
      "Expect low variance and incorrect mean\n",
      "Computed mean MEAN Parameters (m,c) = (1.037,1.490) \n",
      "Computed variance of parameters (v1,v2) = %s [[ 0.13205417 -0.07164857]\n",
      " [-0.07164857  0.04887333]]\n",
      "---***---Weighing smoothly over all prior space---***---\n",
      "Computed mean MEAN Parameters (m,c) = (1.010,1.490) \n",
      "Computed variance of parameters (v1,v2) = %s [[ 0.11297863 -0.06069791]\n",
      " [-0.06069791  0.04251102]]\n",
      "[ 1.03718672  1.49021244]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example starter code for Bayesian linear regression implementations.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "def testing_map():\n",
    "    print 'Testing map'\n",
    "    c,m = 1,1\n",
    "    X = [[np.random.uniform(0,1),1] for _ in range(100)]\n",
    "    Y = [m*each[0]+c+np.random.uniform(0,1) for each in X]    \n",
    "    X,Y = np.array(X),np.array(Y)\n",
    "    emissionvar_scalar = 1\n",
    "    print \"Prior's Influence\"\n",
    "    print 'Original Parameters (m,c) = (%d,%d)' %(m,c)\n",
    "    print '---***---Weighing too much over an incorrect prior, High variance---***---'\n",
    "    print 'Expect low variance and incorrect mean'\n",
    "    Mean, Covar = linreg_post(X, Y, [0,0], 100, emissionvar_scalar)\n",
    "    print 'Computed mean MEAN Parameters (m,c) = (%.3f,%.3f) '  %(Mean[0], Mean[1])\n",
    "    print 'Computed variance of parameters (v1,v2) = %s' , Covar\n",
    "    \n",
    "    print '---***---Weighing smoothly over all prior space---***---'\n",
    "    Mean, Covar = linreg_post(X, Y, [0,0], 1, emissionvar_scalar)\n",
    "    print 'Computed mean MEAN Parameters (m,c) = (%.3f,%.3f) '  %(Mean[0], Mean[1])\n",
    "    print 'Computed variance of parameters (v1,v2) = %s' , Covar\n",
    "    print linreg_mle(X,Y)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def testing_mle():\n",
    "    c = np.random.uniform(0,1)\n",
    "    m = np.random.uniform(0,1)/(np.random.uniform(0,1)+0.001)\n",
    "    n = np.random.uniform(0,1)/(np.random.uniform(0,1)+0.001)\n",
    "    X = [[np.random.uniform(0,1), np.random.uniform(0,1)] for _ in range(100)]\n",
    "    Y = [m*each[0]+n*each[1]+c for each in X]    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    vals1 = list(lr.coef_)+[lr.intercept_, ]\n",
    "    X = [each+ [1,] for each in X]  #Constant term\n",
    "    X,Y = np.array(X), np.array(Y)\n",
    "    vals2 =  linreg_mle(X,Y)\n",
    "    assert(len(vals1) == len(vals2))\n",
    "    for x,y in zip(vals1,vals2): assert(abs(x-y)<0.01)\n",
    "    \n",
    "\n",
    "def linreg_mle(X,Y):\n",
    "    # returns one vector of the inferred MLE weights.\n",
    "    # X: matrix shape (N x J)\n",
    "    # Y: vector length N\n",
    "    N = X.shape[0]\n",
    "    J = X.shape[1]\n",
    "    assert N==len(Y)\n",
    "    P = np.linalg.inv(np.dot(X.transpose(),X))\n",
    "    Q = np.dot(X.transpose(),Y)\n",
    "    return np.dot(P,Q) \n",
    "    # TODO\n",
    "\n",
    "\n",
    "def linreg_post(X,Y, priormean, priorvar_scalar, emissionvar_scalar):\n",
    "    # returns (PosteriorMean, PosteriorCovar)\n",
    "    # where PosteriorMean is a J-length vector\n",
    "    # where PosteriorCovar is a (J x J) shaped matrix\n",
    "    N = X.shape[0]\n",
    "    J = X.shape[1]\n",
    "    assert N==len(Y)\n",
    "    V_0 = priorvar_scalar*np.identity(J)\n",
    " \n",
    "    PosteriorCovar = emissionvar_scalar*np.linalg.inv((np.linalg.inv(emissionvar_scalar*V_0) + np.dot(X.transpose(),X)))\n",
    "    term1 = np.dot(np.dot(PosteriorCovar,np.linalg.inv(emissionvar_scalar*V_0)),priormean)\n",
    "    term2 = np.dot( np.dot(PosteriorCovar,X.transpose()),Y)\n",
    "#     PosteriorMean = term1 + 1/emissionvar_scalar * term2\n",
    "    PosteriorMean = term1 +  term2\n",
    "    return PosteriorMean, PosteriorCovar\n",
    "\n",
    "def cook_linreg(priormean, priorvar_scalar, emissionvar_scalar, Nsim):\n",
    "    \"\"\"Runs Nsim simulations and returns list length Nsim of results for each one.\n",
    "    Each result might contain the posterior mean/covar calculation,\n",
    "    and/or the marginal posterior CDF of the \"true\" weight variable you're\n",
    "    interested in, etc.\n",
    "    (obviously there are lots of alternative ways to structure this.)\n",
    "    \"\"\"\n",
    "    X = np.array([ [1,0], [1,0], [0,0] ])\n",
    "    N = X.shape[0]\n",
    "    J = X.shape[1]\n",
    "    assert J==len(priormean)\n",
    "    V_0 = priorvar_scalar*np.identity(J)\n",
    "    from collections import defaultdict\n",
    "    d = defaultdict(float)\n",
    "    samplesize = 1000\n",
    "    for _ in range(Nsim):\n",
    "        weights  = np.random.multivariate_normal(priormean , V_0 ,1).T\n",
    "        #print weights.shape\n",
    "        #print np.dot(weights.transpose(),X[0].transpose())\n",
    "        Y = []\n",
    "        for x in X : Y.extend(np.random.multivariate_normal(np.dot(weights.transpose(),x.transpose()) , emissionvar_scalar*np.identity(1) ,1).T[0])\n",
    "        Y = np.array(Y)\n",
    "        #print X.shape        \n",
    "        #print Y.shape        \n",
    "        PM, PC = linreg_post(X,Y, priormean, priorvar_scalar, emissionvar_scalar)\n",
    "        #print PM\n",
    "        x,_ = np.random.multivariate_normal(PM , PC ,samplesize).T    \n",
    "        s = 0\n",
    "        for e in x: \n",
    "            if e<weights[0]: \n",
    "                s+=1\n",
    "        s/=samplesize\n",
    "        d[int(s*10)/10]+=1\n",
    "        #d[(int(weights[0]*10)/10)]+=s\n",
    "        \n",
    "    x,y = zip(*sorted(d.items()))\n",
    "    plt.title('Cooks Test')\n",
    "    plt.plot(x,y,'bo')\n",
    "    plt.show()\n",
    "#     from collections import defaultdict\n",
    "#     d = defaultdict(float)\n",
    "#     granularity = 4\n",
    "#     for each in res: d[int(each*granularity)]+=1\n",
    "\n",
    "#     prevsum = 0\n",
    "#     for e in sorted(d.keys()):\n",
    "#         d[e]+= prevsum\n",
    "#         prevsum = d[e]\n",
    "    #print d\n",
    "    #print d[sorted(d.keys())[-1]]\n",
    "    #s_d  = [e*1.0/granularity for e in sorted(d.keys())]\n",
    "    \n",
    "    \n",
    "#     plt.plot(s_d,[d[e*granularity]*1.0/Nsim for e in s_d],'r-')\n",
    "#     plt.plot(s_d,[normcdf(e,priormean[0],priorvar_scalar) for e in s_d],'b-')\n",
    "#     plt.ylim(0,1)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    \n",
    "#     for i in range(10):\n",
    "#         print 'Testing 1.1 %dth  time ... ' %i\n",
    "#         testing_mle()\n",
    "    testing_map()\n",
    "    cook_linreg([0,0], 10.0, 4.0, 10000)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def testing_mle():\n",
    "    c = np.random.uniform(0,1)\n",
    "    m = np.random.uniform(0,1)/(np.random.uniform(0,1)+0.001)\n",
    "    n = np.random.uniform(0,1)/(np.random.uniform(0,1)+0.001)\n",
    "    X = [[np.random.uniform(0,1), np.random.uniform(0,1)] for _ in range(100)]\n",
    "    Y = [m*each[0]+n*each[1]+c for each in X]    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X,Y)\n",
    "    vals1 = list(lr.coef_)+[lr.intercept_, ]\n",
    "    X = [each+ [1,] for each in X]  #Constant term\n",
    "    X,Y = np.array(X), np.array(Y)\n",
    "    vals2 =  linreg_mle(X,Y)\n",
    "    assert(len(vals1) == len(vals2))\n",
    "    for x,y in zip(vals1,vals2): assert(abs(x-y)<0.01)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "\"\"\"\n",
    "1d Normal functions:\n",
    "  - sample from truncated normal\n",
    "  - CDF and inverse CDF for 1d normal\n",
    "\n",
    "2016-04-11: I (brendan) put this together because the scipy.stats versions are\n",
    "SLOW which is really bad for Gibbs sampling.\n",
    "\n",
    "CHECK: yes they give the same answer.\n",
    "\n",
    "In [180]: scipy.stats.norm(1,2).cdf(0)\n",
    "Out[180]: 0.30853753872598688\n",
    "\n",
    "In [181]: norm1d.normcdf(0,1,2)\n",
    "Out[181]: 0.3085375372760062\n",
    "\n",
    "TIMING: omg!!!  one millisecond is way too slow for this.\n",
    "\n",
    "In [184]: timeit scipy.stats.norm(1,2).cdf(0)\n",
    "1000 loops, best of 3: 1.36 ms per loop\n",
    "\n",
    "In [185]: timeit norm1d.cdf(0,1,2)\n",
    "100000 loops, best of 3: 2.03 Âµs per loop\n",
    "\n",
    "This is >500 times faster.  scipy sucks, once again\n",
    "note that the gaussian inverse CDF is generally considered to be slow; that's\n",
    "the 2 microseconds there.\n",
    "\n",
    "CDF and invCDF implementations from\n",
    "http://www.johndcook.com/blog/python_phi_inverse/\n",
    "http://www.johndcook.com/blog/python_phi/\n",
    "\n",
    "and those are probably slow compared to C versions!\n",
    "todo: should switch to math.erf() where possible which will be faster.\n",
    "see examples on\n",
    "http://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def truncnormal(mean, sd, L,R):\n",
    "    \"\"\"\n",
    "    sample from truncated normal, trunctated to range [L,R]\n",
    "    NOTE this takes STD DEV (**not** variance!)\n",
    "    use L=0, R=100 to pretty-good approximate [0,+inf)\n",
    "    pg21 of https://www2.stat.duke.edu/courses/Fall03/sta216/lecture10.pdf\n",
    "    \"\"\"\n",
    "    assert R >= L\n",
    "    L,R = phi((L-mean)/sd), phi((R-mean)/sd)\n",
    "    r = np.random.random() * (R-L) + L\n",
    "    return mean+sd*normal_CDF_inverse(r)\n",
    " \n",
    "def normcdf(x, mean=0.0, sd=1.0):\n",
    "    return phi( (x-mean)/sd )\n",
    " \n",
    "def phi(x):\n",
    "    \"\"\"CDF for N(0,1)\"\"\"\n",
    "    # constants\n",
    "    a1 =  0.254829592\n",
    "    a2 = -0.284496736\n",
    "    a3 =  1.421413741\n",
    "    a4 = -1.453152027\n",
    "    a5 =  1.061405429\n",
    "    p  =  0.3275911\n",
    "\n",
    "    # Save the sign of x\n",
    "    sign = 1\n",
    "    if x < 0:\n",
    "        sign = -1\n",
    "    x = abs(x)/math.sqrt(2.0)\n",
    "\n",
    "    # A&S formula 7.1.26\n",
    "    t = 1.0/(1.0 + p*x)\n",
    "    y = 1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*math.exp(-x*x)\n",
    "\n",
    "    return 0.5*(1.0 + sign*y)\n",
    " \n",
    "def rational_approximation(t):\n",
    "    # Abramowitz and Stegun formula 26.2.23.\n",
    "    # The absolute value of the error should be less than 4.5 e-4.\n",
    "    c = [2.515517, 0.802853, 0.010328]\n",
    "    d = [1.432788, 0.189269, 0.001308]\n",
    "    numerator = (c[2]*t + c[1])*t + c[0]\n",
    "    denominator = ((d[2]*t + d[1])*t + d[0])*t + 1.0\n",
    "    return t - numerator / denominator\n",
    " \n",
    "def normal_CDF_inverse(p):\n",
    "    assert p > 0.0 and p < 1\n",
    " \n",
    "    if p < 0.5:\n",
    "        # F^-1(p) = - G^-1(p)\n",
    "        return -rational_approximation( math.sqrt(-2.0*math.log(p)) )\n",
    "    else:\n",
    "        # F^-1(p) = G^-1(1-p)\n",
    "        return rational_approximation( math.sqrt(-2.0*math.log(1.0-p)) )\n",
    "\n",
    "def normal_CDF_inverse_demo():\n",
    " \n",
    "    print \"\\nShow that the NormalCDFInverse function is accurate at\"\n",
    "    print \"0.05, 0.15, 0.25, ..., 0.95 and at a few extreme values.\\n\\n\"\n",
    " \n",
    "    p = [\n",
    "        0.0000001,\n",
    "        0.00001,\n",
    "        0.001,\n",
    "        0.05,\n",
    "        0.15,\n",
    "        0.25,\n",
    "        0.35,\n",
    "        0.45,\n",
    "        0.55,\n",
    "        0.65,\n",
    "        0.75,\n",
    "        0.85,\n",
    "        0.95,\n",
    "        0.999,\n",
    "        0.99999,\n",
    "        0.9999999\n",
    "    ]\n",
    " \n",
    "    # Exact values computed by Mathematica.\n",
    "    exact = [\n",
    "        -5.199337582187471,\n",
    "        -4.264890793922602,\n",
    "        -3.090232306167813,\n",
    "        -1.6448536269514729,\n",
    "        -1.0364333894937896,\n",
    "        -0.6744897501960817,\n",
    "        -0.38532046640756773,\n",
    "        -0.12566134685507402,\n",
    "         0.12566134685507402,\n",
    "         0.38532046640756773,\n",
    "         0.6744897501960817,\n",
    "         1.0364333894937896,\n",
    "         1.6448536269514729,\n",
    "         3.090232306167813,\n",
    "         4.264890793922602,\n",
    "         5.199337582187471\n",
    "    ]\n",
    " \n",
    "    maxerror = 0.0\n",
    "    num_values = len(p)\n",
    "    print \"p, exact CDF inverse, computed CDF inverse, diff\\n\\n\";\n",
    "     \n",
    "    for i in range(num_values):\n",
    "        computed = normal_CDF_inverse(p[i])\n",
    "        error = exact[i] - computed\n",
    "        print p[i], \",\", exact[i], \",\", computed, \",\", error\n",
    "        if (abs(error) > maxerror):\n",
    "            maxerror = abs(error)\n",
    " \n",
    "    print \"\\nMaximum error:\" , maxerror , \"\\n\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print normcdf(70,10,10)\n",
    "#     plt.plot([normcdf(x/10.0) for x in range(-1000,1000)], [x/10.0 for x in range(-1000,1000)],'b-')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
